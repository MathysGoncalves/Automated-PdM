{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 14:39:06.099043: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 14:39:07.686449: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 14:39:07.687170: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 14:39:07.687200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from modeling import *\n",
    "from data_import import *\n",
    "from data_preparation import *\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global db_config\n",
    "db_config = {                       #Start server first\n",
    "    'host': 'localhost',\n",
    "    'database': 'python_db',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres'\n",
    "}\n",
    "\n",
    "train_table_name = 'train_Bearing_Nasa'\n",
    "test_table_name = 'test_Bearing_Nasa'\n",
    "dow, month = None, None                                                             #! Get from UI\n",
    "cat = None                                                                          #! Get from UI\n",
    "mult_var = None                                                                     #! Get from UI\n",
    "l_periods = None                                                                    #! Get from UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(train_table_name, db_config, dow=None, month=None, cat=None, mult_var=None, l_periods=None):\n",
    "  df = read_table_from_postgres(train_table_name, db_config)\n",
    "  df = data_prep(df, dow=dow, month=month, cat=cat, mult_var=mult_var, l_periods=l_periods)\n",
    "  if_model = fit_IF(df)\n",
    "  df_cleaned = predict_IF(if_model, df)\n",
    "  fit_scaler(df_cleaned, train_table_name)\n",
    "  df_scaled = apply_scaler(df_cleaned, train_table_name)\n",
    "\n",
    "  #set_global(df_cleaned.shape[1])\n",
    "  ae_fit(df_scaled, train_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Time periods...\n",
      "(736, 4)\n",
      "Removing date\n",
      "(736, 4)\n",
      "Fixing Typos...\n",
      "(736, 4)\n",
      "Encoding categorical varible(s)...\n",
      "(736, 4)\n",
      "Bearing 1    float64\n",
      "Bearing 2    float64\n",
      "Bearing 3    float64\n",
      "Bearing 4    float64\n",
      "dtype: object\n",
      "Process Nan...\n",
      "Bearing 1 - 0%\n",
      "Bearing 2 - 0%\n",
      "Bearing 3 - 0%\n",
      "Bearing 4 - 0%\n",
      "(736, 4)\n",
      "End of preprocessing\n",
      "\n",
      "0.1854   -     0.7256   -     0.4772   -     0.7302   -     0.115   -     0.3626   -     0.2111   -     0.7399   -     0.4772   -     0.7305   -     0.1691   -     0.3893   -     0.2086   -     0.6876   -     0.4772   -     0.6774   -     0.1641   -     0.4135   -     0.264   -     0.7853   -     0.4772   -     0.7764   -     0.0881   -     0.338   -     0.2193   -     0.728   -     0.4772   -     0.7193   -     0.1475   -     0.3785   -     0.1827   -     0.6982   -     0.4772   -     0.7043   -     0.1563   -     0.3883   -     0.1943   -     0.7101   -     0.4772   -     0.7017   -     0.1436   -     0.4163   -     0.2581   -     0.7999   -     0.4481   -     0.7764   -     0.1048   -     0.3335   -     0.1783   -     0.6876   -     0.4772   -     0.6766   -     0.2032   -     0.4135   -     0.2831   -     0.7879   -     0.4481   -     0.777   -     0.0549   -     0.3281   -     \n",
      "Optimum parameters {'n_estimators': 490, 'contamination': 0.02, 'bootstrap': False}\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 7.1930e-04\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0069\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0783\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 5.2294e-04\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2032\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1037\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0783\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1084\n",
      "{'num_hidden_layers': 1, 'learning_rate': 0.001, 'input_dim': 4, 'hidden_layer_sizes': 64, 'dropout_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "df = train_pipeline(train_table_name, db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipeline(train_table_name, test_table_name):\n",
    "    df_train = read_table_from_postgres(train_table_name, db_config)\n",
    "    df_test = read_table_from_postgres(test_table_name, db_config)\n",
    "    df_train = df_train.drop_duplicates(keep='last')\n",
    "    df_test = df_test.drop_duplicates(keep='last')\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    df_train_scaled = apply_scaler(df_train, train_table_name)\n",
    "    df_test_scaled = apply_scaler(df_test, train_table_name)\n",
    "\n",
    "    ae_path = \"../models/ae_\" + str(train_table_name) + \".h5\"\n",
    "    model = keras.models.load_model(ae_path)\n",
    "\n",
    "    result = cluster_data(model, df_train_scaled, df_test_scaled, df_test, test_table_name)\n",
    "    write_dataframe_to_postgres(result, str(test_table_name) + \"_Result\", db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "\n",
      "Global Reconstruction error threshold:  0.2898632982504219\n",
      "Successfully wrote 246 rows to table test_Bearing_Nasa_Result in database python_db!\n"
     ]
    }
   ],
   "source": [
    "predict_pipeline(train_table_name, test_table_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a033de9e094f5fb838607587be6d707ecb4c452b8fa81660fc7998914eb78384"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
